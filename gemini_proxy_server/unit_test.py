import unittest
import json
import time
import os
import sys
from unittest.mock import patch
import requests
import threading
import google.generativeai as genai

# Add the current directory to the path to import our modules
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import Config
from database import get_db, engine
from models import RequestLog, ResponseLog
from app import app

class TestGeminiProxyServer(unittest.TestCase):
    """Comprehensive unit tests for Gemini Proxy Server"""
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment"""
        cls.app = app
        cls.app.config['TESTING'] = True
        cls.client = cls.app.test_client()
        cls.server_url = 'http://localhost:5000'
        
        # Start the Flask app in a separate thread for integration tests
        cls.server_thread = threading.Thread(
            target=lambda: cls.app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)
        )
        cls.server_thread.daemon = True
        cls.server_thread.start()
        time.sleep(2)  # Give server time to start
    
    def test_1_api_key_setup(self):
        """Test 1: API Key Setup Test - Verify API key is configured"""
        print("\n=== Test 1: API Key Setup ===")
        
        # Test that API key is loaded from environment
        self.assertIsNotNone(Config.GEMINI_API_KEY, "GEMINI_API_KEY should be set in environment")
        self.assertNotEqual(Config.GEMINI_API_KEY, "", "GEMINI_API_KEY should not be empty")
        self.assertTrue(len(Config.GEMINI_API_KEY) > 10, "GEMINI_API_KEY should be a valid length")
        
        # Test that models list is configured
        self.assertIsInstance(Config.MODELS, list, "MODELS should be a list")
        self.assertGreater(len(Config.MODELS), 0, "MODELS list should not be empty")
        
        # Test that default model is set
        self.assertIsNotNone(Config.MODEL_NAME, "MODEL_NAME should be set")
        self.assertIn(Config.MODEL_NAME, Config.MODELS, "MODEL_NAME should be in MODELS list")
        
        print(f"‚úì API Key configured: {Config.GEMINI_API_KEY[:10]}...")
        print(f"‚úì Models configured: {Config.MODELS}")
        print(f"‚úì Default model: {Config.MODEL_NAME}")
    
    def test_2_gemini_model_response(self):
        """Test 2: Gemini Model Response Test - Direct API test"""
        print("\n=== Test 2: Gemini Model Response ===")
        
        # Configure Gemini with our API key
        genai.configure(api_key=Config.GEMINI_API_KEY)
        
        # Test model availability
        try:
            models = genai.list_models()
            available_model_names = [model.name for model in models]
            print(f"‚úì Available models: {available_model_names}")
            
            # Check if our configured models are available
            for model_name in Config.MODELS:
                if any(model_name in available for available in available_model_names):
                    print(f"‚úì Configured model {model_name} is available")
                    break
            else:
                self.fail("None of the configured models are available")
                
        except Exception as e:
            self.fail(f"Failed to list models: {str(e)}")
        
        # Test actual model response
        test_prompt = "Say 'Hello, test!' in exactly those words."
        successful_model = None
        
        for model_name in Config.MODELS:
            try:
                print(f"Testing model: {model_name}")
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(test_prompt)
                
                # Extract response text
                response_text = ""
                if hasattr(response, 'text'):
                    response_text = response.text
                elif hasattr(response, 'candidates') and response.candidates:
                    for part in response.candidates[0].content.parts:
                        if hasattr(part, 'text'):
                            response_text += part.text
                
                self.assertIsNotNone(response_text, f"Response text should not be None for {model_name}")
                self.assertTrue(len(response_text) > 0, f"Response text should not be empty for {model_name}")
                
                print(f"‚úì Model {model_name} response: {response_text[:50]}...")
                successful_model = model_name
                break
                
            except Exception as e:
                print(f"‚ö† Model {model_name} failed: {str(e)}")
                continue
        
        self.assertIsNotNone(successful_model, "At least one model should work")
        print(f"‚úì Successfully tested model: {successful_model}")
    
    def test_3_database_connection(self):
        """Test 3: Database Connection Test - Verify database setup"""
        print("\n=== Test 3: Database Connection ===")
        
        # Test database connection
        try:
            db = next(get_db())
            print("‚úì Database connection established")
            
            # Test that we can query the database
            request_count = db.query(RequestLog).count()
            response_count = db.query(ResponseLog).count()
            
            print(f"‚úì Request logs table accessible (current count: {request_count})")
            print(f"‚úì Response logs table accessible (current count: {response_count})")
            
            # Test that we can create a test record (and clean it up)
            test_request = RequestLog(
                endpoint="test",
                client_ip="127.0.0.1",
                request_body='{"test": true}'
            )
            db.add(test_request)
            db.commit()
            db.refresh(test_request)
            
            self.assertIsNotNone(test_request.id, "Test request should have an ID")
            print(f"‚úì Can create records (test ID: {test_request.id})")
            
            # Clean up test record
            db.delete(test_request)
            db.commit()
            print("‚úì Can delete records (cleanup successful)")
            
            db.close()
            
        except Exception as e:
            self.fail(f"Database connection test failed: {str(e)}")
    
    def test_4_gemini_generate_endpoint(self):
        """Test 4: /gemini/generate endpoint with valid requests"""
        print("\n=== Test 4: /gemini/generate Endpoint ===")
        
        # Test data
        test_request = {
            "prompt": {
                "text": "Write exactly 'API test successful' and nothing else."
            },
            "generation_config": {
                "temperature": 0.1,
                "max_output_tokens": 50
            }
        }
        
        # Store test data for next test (do this early)
        self.test_prompt_text = test_request["prompt"]["text"]
        
        try:
            # Make request to the endpoint
            response = requests.post(
                f"{self.server_url}/gemini/generate",
                json=test_request,
                headers={'Content-Type': 'application/json'},
                timeout=30
            )
            
            print(f"‚úì HTTP Status: {response.status_code}")
            self.assertEqual(response.status_code, 200, "Should return 200 OK")
            
            # Parse response
            response_data = response.json()
            self.assertIn('text', response_data, "Response should contain 'text' field")
            self.assertIn('metrics', response_data, "Response should contain 'metrics' field")
            self.assertIn('model', response_data, "Response should contain 'model' field")
            
            print(f"‚úì Response text: {response_data['text'][:100]}...")
            print(f"‚úì Model used: {response_data['model']}")
            print(f"‚úì Metrics: {response_data['metrics']}")
            
            # Validate metrics structure
            metrics = response_data['metrics']
            self.assertIn('prompt_tokens', metrics, "Metrics should contain prompt_tokens")
            self.assertIn('completion_tokens', metrics, "Metrics should contain completion_tokens")
            self.assertIn('total_tokens', metrics, "Metrics should contain total_tokens")
            
            # Store response for next test
            self.last_response = response_data
            
        except requests.exceptions.RequestException as e:
            self.fail(f"Request failed: {str(e)}")
        except json.JSONDecodeError as e:
            self.fail(f"Failed to parse JSON response: {str(e)}")
        except Exception as e:
            self.fail(f"Endpoint test failed: {str(e)}")
    
    def test_5_database_logging_verification(self):
        """Test 5: Verify response is updated in database"""
        print("\n=== Test 5: Database Logging Verification ===")
        
        # Wait a moment for database operations to complete
        time.sleep(1)
        
        try:
            db = next(get_db())
            
            # Get the most recent request log
            latest_request = db.query(RequestLog).order_by(RequestLog.id.desc()).first()
            self.assertIsNotNone(latest_request, "Should have at least one request log")
            
            print(f"‚úì Latest request log ID: {latest_request.id}")
            print(f"‚úì Request endpoint: {latest_request.endpoint}")
            print(f"‚úì Request timestamp: {latest_request.timestamp}")
            
            # Verify request contains our test data (with fallback)
            request_body = latest_request.request_body
            if hasattr(self, 'test_prompt_text'):
                self.assertIn(self.test_prompt_text, request_body, "Request body should contain our test prompt")
                print(f"‚úì Request body contains expected prompt")
            else:
                print("‚ö† test_prompt_text not available, skipping prompt verification")
                # Just verify it looks like our test request
                self.assertIn("API test successful", request_body, "Request body should contain test prompt")
            
            # Get the corresponding response log
            response_log = db.query(ResponseLog).filter(
                ResponseLog.request_id == latest_request.id
            ).first()
            self.assertIsNotNone(response_log, "Should have a corresponding response log")
            
            print(f"‚úì Response log ID: {response_log.id}")
            print(f"‚úì Response timestamp: {response_log.timestamp}")
            print(f"‚úì Model used: {response_log.model_used}")
            print(f"‚úì Response time: {response_log.response_time_ms}ms")
            
            # Verify response metrics match what was returned by API (with fallback)
            if hasattr(self, 'last_response'):
                api_metrics = self.last_response['metrics']
                
                self.assertEqual(
                    response_log.prompt_tokens, 
                    api_metrics['prompt_tokens'], 
                    "Database prompt_tokens should match API response"
                )
                self.assertEqual(
                    response_log.completion_tokens, 
                    api_metrics['completion_tokens'], 
                    "Database completion_tokens should match API response"
                )
                self.assertEqual(
                    response_log.total_tokens, 
                    api_metrics['total_tokens'], 
                    "Database total_tokens should match API response"
                )
                
                print(f"‚úì Prompt tokens: {response_log.prompt_tokens}")
                print(f"‚úì Completion tokens: {response_log.completion_tokens}")
                print(f"‚úì Total tokens: {response_log.total_tokens}")
                print("‚úì Metrics match between API response and database")
            else:
                print("‚ö† last_response not available, checking database metrics only")
                print(f"‚úì Prompt tokens: {response_log.prompt_tokens}")
                print(f"‚úì Completion tokens: {response_log.completion_tokens}")
                print(f"‚úì Total tokens: {response_log.total_tokens}")
                
                # At least verify metrics are reasonable numbers
                self.assertGreaterEqual(response_log.prompt_tokens, 0, "Prompt tokens should be >= 0")
                self.assertGreaterEqual(response_log.completion_tokens, 0, "Completion tokens should be >= 0")
                self.assertGreaterEqual(response_log.total_tokens, 0, "Total tokens should be >= 0")
            
            # Verify response text is stored
            self.assertIsNotNone(response_log.response, "Response text should be stored")
            self.assertTrue(len(response_log.response) > 0, "Response text should not be empty")
            
            print(f"‚úì Response text stored: {response_log.response[:50]}...")
            
            db.close()
            
        except Exception as e:
            self.fail(f"Database verification failed: {str(e)}")

def run_tests():
    """Run all tests with detailed output"""
    print("=" * 60)
    print("GEMINI PROXY SERVER UNIT TESTS")
    print("=" * 60)
    
    # Create test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestGeminiProxyServer)
    
    # Run tests with verbose output
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Print summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.failures:
        print("\nFAILURES:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback}")
    
    if result.errors:
        print("\nERRORS:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback}")
    
    if result.wasSuccessful():
        print("\nüéâ ALL TESTS PASSED! üéâ")
    else:
        print("\n‚ùå SOME TESTS FAILED")
    
    return result.wasSuccessful()

if __name__ == '__main__':
    success = run_tests()